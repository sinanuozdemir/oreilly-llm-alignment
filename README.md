# Alignment and Reinforcement Learning with Large Language Models (LLMs)


![O'Reilly](images/oreilly.png)


This repository contains Jupyter notebooks for the courses ["Aligning Large Language Models"](https://www.oreilly.com/live-events/aligning-large-language-models/0636920098043/0636920098042/) and ["Reinforcement Learning with Large Language Models"](https://www.oreilly.com/live-events/reinforcement-learning-with-large-language-models/0636920095685) by Sinan Ozdemir. Published by Pearson, the course covers effective best practices and industry case studies in using Large Language Models (LLMs).

## Aligning Large Language Models

- In-depth exploration of various alignment techniques with hands-on case studies, such as Constitutional AI
- Comprehensive coverage of evaluating alignment, offering specific tools and metrics for continuous assessment and adaptation of LLM alignment strategies
- A focus on ethical considerations and future directions, ensuring participants not only understand the current landscape but are also prepared for emerging trends and challenges in LLM alignment

This class is an intensive exploration into the alignment of Large Language Models (LLMs), a vital topic in modern AI development. Through a combination of theoretical insights and hands-on practice, participants will be exposed to various alignment techniques, including a focus on Constitutional AI, constructing reward mechanisms from human feedback, and instructional alignment. The course will also provide detailed guidance on evaluating alignment, with specific tools and metrics to ensure that models align with desired goals, ethical standards, and real-world applications.

### Course Set-Up

- Jupyter notebooks can be run alongside the instructor, but you can also follow along without coding by viewing pre-run notebooks here.

### Notebooks

- `rlaif.ipynb`: [Investigating Principles of RLAIF and Constituional AI](notebooks/rlaif.ipynb)
- `Instruction Aligning Llama 3.1 8B`: A workshop in fine-tuning Llama 3.1 8B with instructional data and incorporating further pre-training to update it's knowledge base
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1sUXME3CcDEqp1eF8j5-z7bdUh2whKvDO?usp=sharing)

## Reinforcement Learning with Large Language Models


- An immersive deep dive into advanced concepts of reinforcement learning in the context of LLMs.
- A practical, hands-on approach to fine-tuning LLMs, with a focus on real-world applications such as generating neutral summaries using T5.
- A unique opportunity to understand and apply innovative concepts like RLHF, RLAIF, and Constitutional AI in reinforcement learning.

This training offers an intensive exploration into the frontier of reinforcement learning techniques with large language models (LLMs). We will explore advanced topics such as Reinforcement Learning with Human Feedback (RLHF), Reinforcement Learning from AI Feedback (RLAIF), and Constitutional AI, and demonstrate practical applications such as fine-tuning open source LLMs like FLAN-T5 and GPT-2. This course is critical for those keen on deepening their understanding of reinforcement learning, its latest trends, and its application to LLMs.

### Course Set-Up

- Jupyter notebooks can be run alongside the instructor, but you can also follow along without coding by viewing pre-run notebooks here.

### Notebooks

**Case Study 1 - Tuning a model with pre-trained off the shelf classifiers**

- `rl_flan_t5_summaries.ipynb`: [Working with FLAN-T5 models using Reinforcement Learning](notebooks/rl_flan_t5_summaries.ipynb) [![Using SAWYER](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wG8lv6drn872HNZHrT7V9kl6JIF1SXpr?usp=sharing)

**Case Study 2 - Building a ChatGPT clone using SFT + RLHF**

- Fine-tuning the instruction model for the SAWYER bot [![Using SAWYER](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1gN7jsUFQTPAj5uFrq06HcSLQSZzT7hZz?usp=sharing) 

- Training a reward model for the SAWYER bot from human preferences [![Using SAWYER](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bVjTzOjXCOM8J6tzgt3LK-D0K-yGWzyI?usp=sharing) 

- Using Reinforcement Learning from Feedback (RLF) to further align SAWYER [![Using SAWYER](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QR_Xf1GsOyChYzReg_JHxsBTrMZ0Vwz6?usp=sharing) 

- Using and Evalauting SAWYER [![Using SAWYER](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xUrIbqyKoEjxNyjNI6iuYuSNMyksypEO?usp=sharing)

 - Adding further knowledge to SAWYER through SFT [![Using SAWYER](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/12JeS96SVLIyY06bzJs96B5PdTt1Pga06?usp=sharing)

**Case Study 3 - Teaching an LLM to reason with GRPO**

 - Fine-tuning Qwen with GRPO on GSM8k [![Using SAWYER](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Cws1IL_T_0_cP0-cHxFA0FEsXYdiAN_8?usp=sharing)


## Further Resources

- [Other Useful Links](https://learning.oreilly.com/playlists/2953f6c7-0e13-49ac-88e2-b951e11388de/)
